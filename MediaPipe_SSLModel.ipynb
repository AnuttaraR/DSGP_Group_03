{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jathusharini/DSGP_Group_03/blob/main/MediaPipe_SSLModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp_qsNAZroAS",
        "outputId": "8dc83c01-9cea-4577-d35e-b0cd2e2208c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (23.1.21)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.8/dist-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (22.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.19.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.5.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (4.38.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.15.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.9.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucZaigsLcMAq",
        "outputId": "e9371dea-3cdc-4aa8-d3d8-6cd1b65de584"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualcam\n",
            "  Downloading pyvirtualcam-0.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyvirtualcam) (1.22.4)\n",
            "Installing collected packages: pyvirtualcam\n",
            "Successfully installed pyvirtualcam-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BQFwU_ceApgh"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import csv\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pyvirtualcam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75L9O6CXql0S"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FDytX1vBu7V"
      },
      "outputs": [],
      "source": [
        "#Running Successfully : Creating Training Numpy Array\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# RoughSSLDataset - 4 people, 5 words, 5 videos each\n",
        "# C:\\Users\\USER\\Documents\\DEGREE MATERIAL (Year 2)\\CM2603 Data Science Group Project\\RoughSSLDataaset upload this to drive\n",
        "video_dir = \"/content/drive/MyDrive/MediaPipe_Data/Dataset_MediaPipe\"\n",
        "\n",
        "# Initialize MediaPipe Hands model\n",
        "with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands:\n",
        "    # Initialize the numpy array to hold the 3D Hand Pose data for each video\n",
        "    dataset = np.zeros((100, 21, 3))\n",
        "\n",
        "    # Loop through each video in the directory\n",
        "    for i, video_file in enumerate(os.listdir(video_dir)):\n",
        "        # Load the video file\n",
        "        cap = cv2.VideoCapture(os.path.join(video_dir, video_file))\n",
        "\n",
        "        # Loop through each frame of the video\n",
        "        while cap.isOpened():\n",
        "            # Read the next frame\n",
        "            ret, image = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert the image to RGB and pass it to the MediaPipe Hands model\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(image)\n",
        "\n",
        "            # Extract the 3D Hand Pose data from the MediaPipe Hands model output\n",
        "            if results.multi_hand_landmarks:\n",
        "                # Loop through each hand landmark in the list\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    # Get the landmarks for the hand\n",
        "                    landmarks = hand_landmarks.landmark\n",
        "                    # Convert the landmarks to a numpy array\n",
        "                    landmarks_array = np.array([[landmark.x, landmark.y, landmark.z] for landmark in landmarks])\n",
        "                    # Store the landmarks in the dataset array\n",
        "                    dataset[i] = landmarks_array\n",
        "\n",
        "                    # Draw the hand landmarks on the image for visualization\n",
        "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            # Show the image\n",
        "            #cv2_imshow(image)\n",
        "            #if cv2.waitKey(5) & 0xFF == 27:\n",
        "                #break\n",
        "\n",
        "        # Release the video capture\n",
        "        cap.release()\n",
        "\n",
        "    # Close the windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Save the dataset to a numpy file\n",
        "np.save(\"/content/drive/MyDrive/MediaPipe_Data/DatasetNPY_MediaPipe/dataset.npy\", dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpFuYnolDcSO"
      },
      "outputs": [],
      "source": [
        "#Running Successfully : Creating Training Labels CSV\n",
        "\n",
        "# Load the dataset numpy array\n",
        "dataset = np.load(\"/content/drive/MyDrive/MediaPipe_Data/DatasetNPY_MediaPipe/dataset.npy\")\n",
        "\n",
        "# Define the path to save the CSV file\n",
        "csv_path = '/content/drive/MyDrive/MediaPipe_Data/LabelsCSV_MediaPipe/labels.csv'\n",
        "\n",
        "# Define the labels for each video in the dataset\n",
        "labels = [\"angry\"] * 20 + [\"bank\"] * 20 + [\"brother\"] * 20 + [\"bye\"] * 20 + [\"excuse_me\"] * 20\n",
        "\n",
        "# Write the labels to a CSV file\n",
        "with open(csv_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"video_id\", \"label\"])\n",
        "    for i, label in enumerate(labels):\n",
        "        writer.writerow([i, label])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Running Successfully : Creating Testing Numpy Array\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# RoughSSLDataset - 1 person, 5 words, 5 videos each\n",
        "# C:\\Users\\USER\\Documents\\DEGREE MATERIAL (Year 2)\\CM2603 Data Science Group Project\\RoughSSLDataaset upload this to drive\n",
        "video_dir = \"/content/drive/MyDrive/MediaPipe_Data/Test_Dataset_MediaPipe\"\n",
        "\n",
        "# Initialize MediaPipe Hands model\n",
        "with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands:\n",
        "    # Initialize the numpy array to hold the 3D Hand Pose data for each video\n",
        "    dataset = np.zeros((25, 21, 3))\n",
        "\n",
        "    # Loop through each video in the directory\n",
        "    for i, video_file in enumerate(os.listdir(video_dir)):\n",
        "        # Load the video file\n",
        "        cap = cv2.VideoCapture(os.path.join(video_dir, video_file))\n",
        "\n",
        "        # Loop through each frame of the video\n",
        "        while cap.isOpened():\n",
        "            # Read the next frame\n",
        "            ret, image = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert the image to RGB and pass it to the MediaPipe Hands model\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(image)\n",
        "\n",
        "            # Extract the 3D Hand Pose data from the MediaPipe Hands model output\n",
        "            if results.multi_hand_landmarks:\n",
        "                # Loop through each hand landmark in the list\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    # Get the landmarks for the hand\n",
        "                    landmarks = hand_landmarks.landmark\n",
        "                    # Convert the landmarks to a numpy array\n",
        "                    landmarks_array = np.array([[landmark.x, landmark.y, landmark.z] for landmark in landmarks])\n",
        "                    # Store the landmarks in the dataset array\n",
        "                    dataset[i] = landmarks_array\n",
        "\n",
        "                    # Draw the hand landmarks on the image for visualization\n",
        "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "        # Release the video capture\n",
        "        cap.release()\n",
        "\n",
        "    # Close the windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Save the dataset to a numpy file\n",
        "np.save(\"/content/drive/MyDrive/MediaPipe_Data/Test_DatasetNPY_MediaPipe/test_dataset.npy\", dataset)\n"
      ],
      "metadata": {
        "id": "ORTEqMGGnSmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running Successfully : Creating New Data Numpy Array\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# RoughSSLDataset - 1 person, 5 words, 5 videos each\n",
        "# C:\\Users\\USER\\Documents\\DEGREE MATERIAL (Year 2)\\CM2603 Data Science Group Project\\RoughSSLDataaset upload this to drive\n",
        "video_dir = \"/content/drive/MyDrive/MediaPipe_Data/NewData_Dataset_MediaPipe\"\n",
        "\n",
        "# Initialize MediaPipe Hands model\n",
        "with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands:\n",
        "    # Initialize the numpy array to hold the 3D Hand Pose data for each video\n",
        "    dataset = np.zeros((1, 21, 3))\n",
        "\n",
        "    # Loop through each video in the directory\n",
        "    for i, video_file in enumerate(os.listdir(video_dir)):\n",
        "        # Load the video file\n",
        "        cap = cv2.VideoCapture(os.path.join(video_dir, video_file))\n",
        "\n",
        "        # Loop through each frame of the video\n",
        "        while cap.isOpened():\n",
        "            # Read the next frame\n",
        "            ret, image = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert the image to RGB and pass it to the MediaPipe Hands model\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(image)\n",
        "\n",
        "            # Extract the 3D Hand Pose data from the MediaPipe Hands model output\n",
        "            if results.multi_hand_landmarks:\n",
        "                # Loop through each hand landmark in the list\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    # Get the landmarks for the hand\n",
        "                    landmarks = hand_landmarks.landmark\n",
        "                    # Convert the landmarks to a numpy array\n",
        "                    landmarks_array = np.array([[landmark.x, landmark.y, landmark.z] for landmark in landmarks])\n",
        "                    # Store the landmarks in the dataset array\n",
        "                    dataset[i] = landmarks_array\n",
        "\n",
        "                    # Draw the hand landmarks on the image for visualization\n",
        "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "        # Release the video capture\n",
        "        cap.release()\n",
        "\n",
        "    # Close the windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Save the dataset to a numpy file\n",
        "np.save(\"/content/drive/MyDrive/MediaPipe_Data/NewDataNPY_MediaPipe/new_data.npy\", dataset)\n"
      ],
      "metadata": {
        "id": "LvhhhVjzKK5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running Successfully : Creating Testing Labels CSV\n",
        "\n",
        "# Load the dataset numpy array\n",
        "dataset = np.load(\"/content/drive/MyDrive/MediaPipe_Data/Test_DatasetNPY_MediaPipe/test_dataset.npy\")\n",
        "\n",
        "# Define the path to save the CSV file\n",
        "csv_path = '/content/drive/MyDrive/MediaPipe_Data/Test_LabelsCSV_MediaPipe/test_labels.csv'\n",
        "\n",
        "# Define the labels for each video in the dataset\n",
        "labels = [\"angry\"] * 5 + [\"bank\"] * 5 + [\"brother\"] * 5 + [\"bye\"] * 5 + [\"excuse_me\"] * 5\n",
        "\n",
        "# Write the labels to a CSV file\n",
        "with open(csv_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"video_id\", \"label\"])\n",
        "    for i, label in enumerate(labels):\n",
        "        writer.writerow([i, label])\n",
        "\n"
      ],
      "metadata": {
        "id": "IgCcBccbnbiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTdvvJj3BwPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c24c98c-19da-480f-fc36-c8697fade3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3/3 [==============================] - 83s 33s/step - loss: 78.8896 - accuracy: 0.1250 - val_loss: 41.9251 - val_accuracy: 0.1500\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 44s 14s/step - loss: 72.6957 - accuracy: 0.3125 - val_loss: 39.5948 - val_accuracy: 0.2000\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 47s 13s/step - loss: 46.7831 - accuracy: 0.3125 - val_loss: 23.8402 - val_accuracy: 0.4000\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 43s 14s/step - loss: 27.7764 - accuracy: 0.3250 - val_loss: 31.5560 - val_accuracy: 0.2500\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 45s 13s/step - loss: 31.0072 - accuracy: 0.3875 - val_loss: 26.3906 - val_accuracy: 0.2500\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 41s 13s/step - loss: 25.2671 - accuracy: 0.3750 - val_loss: 15.8936 - val_accuracy: 0.3000\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 41s 13s/step - loss: 12.7477 - accuracy: 0.4125 - val_loss: 16.4372 - val_accuracy: 0.1500\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 43s 14s/step - loss: 12.4285 - accuracy: 0.4375 - val_loss: 9.7004 - val_accuracy: 0.3500\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 46s 15s/step - loss: 8.2689 - accuracy: 0.4375 - val_loss: 6.7369 - val_accuracy: 0.4500\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 42s 13s/step - loss: 7.6198 - accuracy: 0.5000 - val_loss: 4.0898 - val_accuracy: 0.6000\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 43s 14s/step - loss: 6.8530 - accuracy: 0.6375 - val_loss: 2.8676 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 47s 15s/step - loss: 4.2100 - accuracy: 0.6500 - val_loss: 5.2830 - val_accuracy: 0.4000\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 41s 13s/step - loss: 4.9720 - accuracy: 0.6250 - val_loss: 3.6356 - val_accuracy: 0.4500\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 41s 13s/step - loss: 3.1021 - accuracy: 0.6000 - val_loss: 3.5078 - val_accuracy: 0.4500\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 43s 14s/step - loss: 2.5294 - accuracy: 0.6375 - val_loss: 7.0009 - val_accuracy: 0.4500\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 45s 13s/step - loss: 2.9246 - accuracy: 0.6500 - val_loss: 2.4167 - val_accuracy: 0.4500\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 41s 13s/step - loss: 1.4339 - accuracy: 0.7500 - val_loss: 2.2300 - val_accuracy: 0.6500\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 41s 13s/step - loss: 1.8863 - accuracy: 0.7000 - val_loss: 2.9434 - val_accuracy: 0.5500\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 43s 14s/step - loss: 1.1954 - accuracy: 0.7750 - val_loss: 2.6685 - val_accuracy: 0.6000\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 46s 13s/step - loss: 0.8472 - accuracy: 0.7750 - val_loss: 1.9471 - val_accuracy: 0.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# Load dataset and labels\n",
        "# Numpy array of shape (num_videos, num_frames, num_joints, 3) for 3D Hand Pose data\n",
        "# and a list of labels of shape (num_videos,)\n",
        "dataset = np.load(\"/content/drive/MyDrive/MediaPipe_Data/DatasetNPY_MediaPipe/dataset.npy\")\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/MediaPipe_Data/LabelsCSV_MediaPipe/labels.csv\")[\"label\"].tolist()\n",
        "\n",
        "# Convert labels to integer encoding\n",
        "le = LabelEncoder()\n",
        "le.fit(labels)\n",
        "labels = le.transform(labels)\n",
        "\n",
        "# Assuming you have labels stored in a 1D numpy array 'labels'\n",
        "num_classes = len(np.unique(labels))\n",
        "\n",
        "# Create an empty array of shape (num_examples, num_classes)\n",
        "one_hot_labels = np.zeros((len(labels), num_classes))\n",
        "\n",
        "# Fill the array with 1s at the appropriate positions\n",
        "for i, label in enumerate(labels):\n",
        "    one_hot_labels[i, label] = 1\n",
        "    \n",
        "# Save the one-hot labels to a file\n",
        "np.save('/content/drive/MyDrive/MediaPipe_Data/LabelsCSV_MediaPipe/one_hot_labels.npy', one_hot_labels)\n",
        "\n",
        "# Preprocess the data\n",
        "# Mean normalization and standard deviation scaling to preprocess the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "dataset = scaler.fit_transform(dataset.reshape(-1, dataset.shape[-1])).reshape(dataset.shape)\n",
        "\n",
        "# Convert one-hot labels to integer labels\n",
        "integer_labels = np.argmax(one_hot_labels, axis=1)\n",
        "one_hot_labels = np.load('/content/drive/MyDrive/MediaPipe_Data/LabelsCSV_MediaPipe/one_hot_labels.npy')\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(dataset, one_hot_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the training and validation data\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1, X_train.shape[2], X_train.shape[3]))\n",
        "X_train = np.tile(X_train, (1, 1, 64, 64, 1))\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1, X_val.shape[2], X_val.shape[3]))\n",
        "X_val = np.tile(X_val, (1, 1, 64, 64, 1))\n",
        "\n",
        "#Using a simple convolutional neural network\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv3D(32, kernel_size=(3,3,3), activation=\"relu\", input_shape=X_train[0].shape),\n",
        "    tf.keras.layers.MaxPooling3D(pool_size=(2,2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
        "\n",
        "#saving the model\n",
        "model.save(\"/content/drive/MyDrive/MediaPipe_Data/Model_MediaPipe/Tf_mp_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on a test set\n",
        "# Load the test dataset and labels\n",
        "test_dataset = np.load(\"/content/drive/MyDrive/MediaPipe_Data/Test_DatasetNPY_MediaPipe/test_dataset.npy\")\n",
        "test_labels = pd.read_csv(\"/content/drive/MyDrive/MediaPipe_Data/Test_LabelsCSV_MediaPipe/test_labels.csv\")[\"label\"].tolist()\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(test_labels)\n",
        "labels = le.transform(test_labels)\n",
        "# Convert test labels to integer encoding\n",
        "test_labels = le.transform(test_labels)\n",
        "\n",
        "# Convert test one-hot labels to integer labels\n",
        "test_one_hot_labels = np.load('/content/drive/MyDrive/MediaPipe_Data/Test_LabelsCSV_MediaPipe/test_one_hot_labels.npy')\n",
        "test_integer_labels = np.argmax(test_one_hot_labels, axis=1)\n",
        "\n",
        "# Mean normalization and standard deviation scaling to preprocess the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "dataset = scaler.fit_transform(test_dataset.reshape(-1, test_dataset.shape[-1])).reshape(test_dataset.shape)\n",
        "\n",
        "# Preprocess the test data\n",
        "test_dataset = scaler.transform(test_dataset.reshape(-1, test_dataset.shape[-1])).reshape(test_dataset.shape)\n",
        "test_dataset = np.expand_dims(test_dataset, axis=-1)\n",
        "test_dataset = np.reshape(test_dataset, (test_dataset.shape[0], test_dataset.shape[1], 1, test_dataset.shape[2], test_dataset.shape[3]))\n",
        "test_dataset = np.tile(test_dataset, (1, 1, 64, 64, 1))\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/MediaPipe_Data/Model_MediaPipe/Tf_mp_model\")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_acc = model.evaluate(test_dataset, test_one_hot_labels)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "print()\n",
        "\n",
        "# # Use the trained model to make predictions\n",
        "# # Numpy array of shape (num_frames, num_joints, 3) for a single video\n",
        "# new_data = np.load(\"/content/drive/MyDrive/MediaPipe_Data/NewDataNPY_MediaPipe/new_data.npy\")\n",
        "\n",
        "# # Reshape and transpose new_data to match expected input shape of the model\n",
        "# new_data = np.transpose(new_data, (0, 2, 1)) # (num_frames, 3, num_joints)\n",
        "# new_data = np.reshape(new_data, (-1, 3, 21, 1)) # (num_frames*21, 3, num_joints, 1)\n",
        "# new_data = np.transpose(new_data, (0, 2, 3, 1)) # (num_frames*21, num_joints, 1, 3)\n",
        "# new_data = np.tile(new_data, (1, 1, 64, 1)) # (num_frames*21, num_joints, 64, 3)\n",
        "\n",
        "# # Preprocess the new data\n",
        "# new_data = scaler.transform(new_data.reshape(-1, new_data.shape[-1])).reshape(new_data.shape)\n",
        "# new_data = np.expand_dims(new_data, axis=-1)\n",
        "# new_data = np.reshape(new_data, (-1, 21, 64, 192, 1))\n",
        "\n",
        "# predictions = model.predict(new_data)\n",
        "# print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGeqZBcppLPe",
        "outputId": "72a9c353-90e0-4fa2-a7d9-d9b6631c5a47"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step - loss: 10.2280 - accuracy: 0.2400\n",
            "Test loss: 10.227957725524902\n",
            "Test accuracy: 0.23999999463558197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "from IPython.display import HTML\n",
        "\n",
        "def show_camera():\n",
        "    js = f\"\"\"\n",
        "    var video = document.querySelector(\"#camera-stream\");\n",
        "    if (navigator.mediaDevices.getUserMedia) {{\n",
        "        navigator.mediaDevices.getUserMedia({{ video: true }})\n",
        "          .then(function (stream) {{\n",
        "            video.srcObject = stream;\n",
        "          }})\n",
        "          .catch(function (err0r) {{\n",
        "            console.log(\"Something went wrong!\");\n",
        "          }});\n",
        "    }}\n",
        "    \"\"\"\n",
        "    display(HTML(\"\"\"\n",
        "    <video id=\"camera-stream\" width=\"640\" height=\"480\" autoplay></video>\n",
        "    \"\"\"))\n",
        "    output.eval_js(js)\n",
        "\n",
        "show_camera()\n"
      ],
      "metadata": {
        "id": "c89wcyZebJbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "EjL0Q3y-RZFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbcefe9e-6ba9-4f45-bd33-a919e493a62a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to read frame from camera\n"
          ]
        }
      ],
      "source": [
        "# Load the trained TensorFlow model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/MediaPipe_Data/Model_MediaPipe/Tf_mp_model')\n",
        "\n",
        "# Create a MediaPipe Hands object\n",
        "mp_hands = mp.solutions.hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# Open the camera\n",
        "cap = cv2.VideoCapture('/dev/video0')\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the camera\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        print(\"Failed to read frame from camera\")\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB format\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the frame with MediaPipe Hands to detect hand landmarks\n",
        "    results = mp_hands.process(frame)\n",
        "\n",
        "    # If hands are detected, extract the landmarks and pass them through the trained model\n",
        "    if results.multi_hand_landmarks:\n",
        "        # Extract the landmarks for the first hand\n",
        "        hand_landmarks = results.multi_hand_landmarks[0]\n",
        "\n",
        "        # Convert the landmarks to a numpy array\n",
        "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
        "\n",
        "        # Add a batch dimension to the landmarks array\n",
        "        landmarks = np.expand_dims(landmarks, axis=0)\n",
        "\n",
        "        # Pass the landmarks through the trained model to predict the sign language word being signed\n",
        "        prediction = model.predict(landmarks)\n",
        "        predicted_word = np.argmax(prediction)\n",
        "\n",
        "        # Draw the predicted word on the frame\n",
        "        cv2.putText(frame, \"Predicted word: {}\".format(predicted_word), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Convert the frame back to BGR format for display\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow(\"Sinhala Sign Language Recognition: \", frame)\n",
        "\n",
        "    # Exit if the user presses the 'q' key\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and destroy the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOOJVAKPVYMw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "5a881eee-7991-440a-9b9b-2979b69704ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-97fd763f838a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MediaPipe_Data/Test_DatasetNPY_MediaPipe/test_dataset.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MediaPipe_Data/Test_LabelsCSV_MediaPipe/test_labels.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;31m# Convert each value according to its column, then pack it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 \u001b[0;31m# according to the dtype's nesting, and store it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m                 \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# The islice is empty, i.e. we're done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mconvert_row\u001b[0;34m(vals, _conv)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;31m# equal dtypes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mconvert_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_conv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mconvert_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_floatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_floatconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The fastest path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'0x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Don't accidentally convert \"a\" (\"0xa\") to 10.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'video_id'"
          ]
        }
      ],
      "source": [
        "# Load the test dataset\n",
        "test_data = np.load('/content/drive/MyDrive/MediaPipe_Data/Test_DatasetNPY_MediaPipe/test_dataset.npy')\n",
        "test_labels = np.loadtxt('/content/drive/MyDrive/MediaPipe_Data/Test_LabelsCSV_MediaPipe/test_labels.csv', delimiter=',')\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/MediaPipe_Data/Model_MediaPipe/Tf_mp_model')\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "test_predictions = np.argmax(model.predict(test_data), axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "\n",
        "# Compute the F1 score\n",
        "f1 = f1_score(test_labels, test_predictions, average='macro')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "print('F1 Score:', f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13koHaRWEcuouQPn-DEapCJICkzA8hILU",
      "authorship_tag": "ABX9TyPR5kMsgVI0csvYhKq4/UzN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}